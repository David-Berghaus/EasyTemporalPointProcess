pipeline_config_id: runner_config

data:
  hawkes_local:
    data_format: json
    train_dir: tests/synthetic_data.json
    valid_dir: tests/synthetic_data.json
    test_dir: tests/synthetic_data.json
    data_specs:
      num_event_types: 3
      pad_token_id: 3
      padding_side: right
      truncation_side: right
      fim_context_size: 16
      fim_inference_size: 16
      fim_max_num_events: 40
      fim_sampling_strategy: sequential
      fim_episodes_per_epoch: 20

FIM_train:
  base_config:
    stage: train
    backend: torch
    dataset_id: hawkes_local
    runner_id: std_tpp
    model_id: FIMHawkesModel
    base_dir: ./checkpoints/
  trainer_config:
    batch_size: 1
    max_epoch: 1
    shuffle: False
    optimizer: adam
    learning_rate: 1.e-4
    valid_freq: 1
    use_tfb: False
    metrics: [ 'acc', 'rmse' ]
    seed: 42
    gpu: 0
  model_config:
    hidden_size: 32
    time_emb_size: 16
    num_layers: 1
    num_heads: 1
    loss_integral_num_sample_per_step: 20
    use_ln: False
    gpu: 0
    model_specs:
      fim_checkpoint_path: /cephfs/users/berghaus/FoundationModels/FIM/results/.ICLR_submission_model/checkpoints/best-model
      fim_sampling_method: thinning
      fim_num_integration_points: 5000
      fim_context_size: 16
      fim_inference_size: 16
      fim_max_num_events: 40
      fim_episodes_per_epoch: 20

FIM_eval:
  base_config:
    stage: eval
    backend: torch
    dataset_id: hawkes_local
    runner_id: std_tpp
    model_id: FIMHawkesModel
    base_dir: ./checkpoints/
  trainer_config:
    batch_size: 1
    max_epoch: 1
    shuffle: False
    optimizer: adam
    learning_rate: 1.e-4
    valid_freq: 1
    use_tfb: False
    metrics: [ 'acc', 'rmse' ]
    seed: 42
    gpu: 0
  model_config:
    hidden_size: 32
    time_emb_size: 16
    num_layers: 1
    num_heads: 1
    loss_integral_num_sample_per_step: 20
    use_ln: False
    gpu: 0
    model_specs:
      fim_checkpoint_path: /cephfs/users/berghaus/FoundationModels/FIM/results/.ICLR_submission_model/checkpoints/best-model
      fim_sampling_method: thinning
      fim_num_integration_points: 5000
      fim_context_size: 16
      fim_inference_size: 16
      fim_max_num_events: 40
      fim_episodes_per_epoch: 20

NHP_train_hawkes:
  base_config:
    stage: train
    backend: torch
    dataset_id: hawkes_local
    runner_id: std_tpp
    model_id: NHP
    base_dir: ./checkpoints/
  trainer_config:
    batch_size: 32
    max_epoch: 3
    shuffle: False
    optimizer: adam
    learning_rate: 1.e-3
    valid_freq: 1
    use_tfb: False
    metrics: [ 'acc', 'rmse' ]
    seed: 42
    gpu: 0
  model_config:
    hidden_size: 64
    loss_integral_num_sample_per_step: 20
    use_ln: False
    gpu: 0
    thinning:
      num_seq: 10
      num_sample: 1
      num_exp: 200
      look_ahead_time: 5
      patience_counter: 3
      over_sample_rate: 5
      num_samples_boundary: 5
      dtime_max: 5

NHP_eval_hawkes:
  base_config:
    stage: eval
    backend: torch
    dataset_id: hawkes_local
    runner_id: std_tpp
    model_id: NHP
    base_dir: ./checkpoints/
  trainer_config:
    batch_size: 32
    max_epoch: 1
    shuffle: False
    optimizer: adam
    learning_rate: 1.e-3
    valid_freq: 1
    use_tfb: False
    metrics: [ 'acc', 'rmse' ]
    seed: 42
    gpu: 0
  model_config:
    hidden_size: 64
    loss_integral_num_sample_per_step: 20
    use_ln: False
    gpu: 0
    thinning:
      num_seq: 10
      num_sample: 1
      num_exp: 200
      look_ahead_time: 5
      patience_counter: 3
      over_sample_rate: 5
      num_samples_boundary: 5
      dtime_max: 5

THP_train_hawkes:
  base_config:
    stage: train
    backend: torch
    dataset_id: hawkes_local
    runner_id: std_tpp
    model_id: THP
    base_dir: ./checkpoints/
  trainer_config:
    batch_size: 32
    max_epoch: 3
    shuffle: False
    optimizer: adam
    learning_rate: 1.e-3
    valid_freq: 1
    use_tfb: False
    metrics: [ 'acc', 'rmse' ]
    seed: 42
    gpu: 0
  model_config:
    hidden_size: 32
    time_emb_size: 16
    num_layers: 2
    num_heads: 2
    mc_num_sample_per_step: 20
    loss_integral_num_sample_per_step: 20
    use_ln: False
    gpu: 0
    thinning:
      num_seq: 10
      num_sample: 1
      num_exp: 200
      look_ahead_time: 5
      patience_counter: 3
      over_sample_rate: 5
      num_samples_boundary: 5
      dtime_max: 5

THP_eval_hawkes:
  base_config:
    stage: eval
    backend: torch
    dataset_id: hawkes_local
    runner_id: std_tpp
    model_id: THP
    base_dir: ./checkpoints/
  trainer_config:
    batch_size: 32
    max_epoch: 1
    shuffle: False
    optimizer: adam
    learning_rate: 1.e-3
    valid_freq: 1
    use_tfb: False
    metrics: [ 'acc', 'rmse' ]
    seed: 42
    gpu: 0
  model_config:
    hidden_size: 32
    time_emb_size: 16
    num_layers: 2
    num_heads: 2
    mc_num_sample_per_step: 20
    loss_integral_num_sample_per_step: 20
    use_ln: False
    gpu: 0
    thinning:
      num_seq: 10
      num_sample: 1
      num_exp: 200
      look_ahead_time: 5
      patience_counter: 3
      over_sample_rate: 5
      num_samples_boundary: 5
      dtime_max: 5

SAHP_train_hawkes:
  base_config:
    stage: train
    backend: torch
    dataset_id: hawkes_local
    runner_id: std_tpp
    model_id: SAHP
    base_dir: ./checkpoints/
  trainer_config:
    batch_size: 32
    max_epoch: 3
    shuffle: False
    optimizer: adam
    learning_rate: 1.e-3
    valid_freq: 1
    use_tfb: False
    metrics: [ 'acc', 'rmse' ]
    seed: 42
    gpu: 0
  model_config:
    hidden_size: 32
    time_emb_size: 16
    num_layers: 2
    num_heads: 2
    loss_integral_num_sample_per_step: 20
    use_ln: False
    gpu: 0
    thinning:
      num_seq: 10
      num_sample: 1
      num_exp: 200
      look_ahead_time: 5
      patience_counter: 3
      over_sample_rate: 5
      num_samples_boundary: 5
      dtime_max: 5

SAHP_eval_hawkes:
  base_config:
    stage: eval
    backend: torch
    dataset_id: hawkes_local
    runner_id: std_tpp
    model_id: SAHP
    base_dir: ./checkpoints/
  trainer_config:
    batch_size: 32
    max_epoch: 1
    shuffle: False
    optimizer: adam
    learning_rate: 1.e-3
    valid_freq: 1
    use_tfb: False
    metrics: [ 'acc', 'rmse' ]
    seed: 42
    gpu: 0
  model_config:
    hidden_size: 32
    time_emb_size: 16
    num_layers: 2
    num_heads: 2
    loss_integral_num_sample_per_step: 20
    use_ln: False
    gpu: 0
    thinning:
      num_seq: 10
      num_sample: 1
      num_exp: 200
      look_ahead_time: 5
      patience_counter: 3
      over_sample_rate: 5
      num_samples_boundary: 5
      dtime_max: 5

AttNHP_train_hawkes:
  base_config:
    stage: train
    backend: torch
    dataset_id: hawkes_local
    runner_id: std_tpp
    model_id: AttNHP
    base_dir: ./checkpoints/
  trainer_config:
    batch_size: 32
    max_epoch: 3
    shuffle: False
    optimizer: adam
    learning_rate: 1.e-3
    valid_freq: 1
    use_tfb: False
    metrics: [ 'acc', 'rmse' ]
    seed: 42
    gpu: 0
  model_config:
    hidden_size: 32
    time_emb_size: 8
    num_layers: 2
    num_heads: 2
    loss_integral_num_sample_per_step: 10
    use_ln: False
    gpu: 0
    thinning:
      num_seq: 5
      num_sample: 1
      num_exp: 100
      look_ahead_time: 5
      patience_counter: 3
      over_sample_rate: 5
      num_samples_boundary: 5
      dtime_max: 5

AttNHP_eval_hawkes:
  base_config:
    stage: eval
    backend: torch
    dataset_id: hawkes_local
    runner_id: std_tpp
    model_id: AttNHP
    base_dir: ./checkpoints/
  trainer_config:
    batch_size: 32
    max_epoch: 1
    shuffle: False
    optimizer: adam
    learning_rate: 1.e-3
    valid_freq: 1
    use_tfb: False
    metrics: [ 'acc', 'rmse' ]
    seed: 42
    gpu: 0
  model_config:
    hidden_size: 32
    time_emb_size: 8
    num_layers: 2
    num_heads: 2
    loss_integral_num_sample_per_step: 10
    use_ln: False
    gpu: 0
    thinning:
      num_seq: 5
      num_sample: 1
      num_exp: 100
      look_ahead_time: 5
      patience_counter: 3
      over_sample_rate: 5
      num_samples_boundary: 5
      dtime_max: 5

FullyNN_train_hawkes:
  base_config:
    stage: train
    backend: torch
    dataset_id: hawkes_local
    runner_id: std_tpp
    model_id: FullyNN
    base_dir: ./checkpoints/
  trainer_config:
    batch_size: 32
    max_epoch: 3
    shuffle: False
    optimizer: adam
    learning_rate: 1.e-3
    valid_freq: 1
    use_tfb: False
    metrics: [ 'acc', 'rmse' ]
    seed: 42
    gpu: 0
  model_config:
    rnn_type: LSTM
    hidden_size: 32
    time_emb_size: 8
    num_layers: 2
    num_heads: 2
    mc_num_sample_per_step: 20
    sharing_param_layer: False
    loss_integral_num_sample_per_step: 20
    dropout: 0.0
    use_ln: False
    gpu: 0
    model_specs:
      num_mlp_layers: 2
      proper_marked_intensities: False

FullyNN_eval_hawkes:
  base_config:
    stage: eval
    backend: torch
    dataset_id: hawkes_local
    runner_id: std_tpp
    model_id: FullyNN
    base_dir: ./checkpoints/
  trainer_config:
    batch_size: 32
    max_epoch: 1
    shuffle: False
    optimizer: adam
    learning_rate: 1.e-3
    valid_freq: 1
    use_tfb: False
    metrics: [ 'acc', 'rmse' ]
    seed: 42
    gpu: 0
  model_config:
    rnn_type: LSTM
    hidden_size: 32
    time_emb_size: 8
    num_layers: 2
    num_heads: 2
    mc_num_sample_per_step: 20
    sharing_param_layer: False
    loss_integral_num_sample_per_step: 20
    dropout: 0.0
    use_ln: False
    gpu: 0
    model_specs:
      num_mlp_layers: 2
      proper_marked_intensities: False

IntensityFree_train_hawkes:
  base_config:
    stage: train
    backend: torch
    dataset_id: hawkes_local
    runner_id: std_tpp
    model_id: IntensityFree
    base_dir: ./checkpoints/
  trainer_config:
    batch_size: 32
    max_epoch: 3
    shuffle: False
    optimizer: adam
    learning_rate: 1.e-3
    valid_freq: 1
    use_tfb: False
    metrics: [ 'acc', 'rmse' ]
    seed: 42
    gpu: 0
  model_config:
    hidden_size: 32
    time_emb_size: 16
    num_layers: 2
    num_heads: 2
    mc_num_sample_per_step: 20
    sharing_param_layer: False
    loss_integral_num_sample_per_step: 20
    dropout: 0.0
    use_ln: False
    gpu: 0
    model_specs:
      num_mix_components: 2
    thinning:
      num_seq: 10
      num_sample: 1
      num_exp: 200
      look_ahead_time: 5
      patience_counter: 3
      over_sample_rate: 5
      num_samples_boundary: 5
      dtime_max: 5

IntensityFree_eval_hawkes:
  base_config:
    stage: eval
    backend: torch
    dataset_id: hawkes_local
    runner_id: std_tpp
    model_id: IntensityFree
    base_dir: ./checkpoints/
  trainer_config:
    batch_size: 32
    max_epoch: 1
    shuffle: False
    optimizer: adam
    learning_rate: 1.e-3
    valid_freq: 1
    use_tfb: False
    metrics: [ 'acc', 'rmse' ]
    seed: 42
    gpu: 0
  model_config:
    hidden_size: 32
    time_emb_size: 16
    num_layers: 2
    num_heads: 2
    mc_num_sample_per_step: 20
    sharing_param_layer: False
    loss_integral_num_sample_per_step: 20
    dropout: 0.0
    use_ln: False
    gpu: 0
    model_specs:
      num_mix_components: 2
    thinning:
      num_seq: 10
      num_sample: 1
      num_exp: 200
      look_ahead_time: 5
      patience_counter: 3
      over_sample_rate: 5
      num_samples_boundary: 5
      dtime_max: 5

RMTPP_train_hawkes:
  base_config:
    stage: train
    backend: torch
    dataset_id: hawkes_local
    runner_id: std_tpp
    model_id: RMTPP
    base_dir: ./checkpoints/
  trainer_config:
    batch_size: 32
    max_epoch: 3
    shuffle: False
    optimizer: adam
    learning_rate: 1.e-3
    valid_freq: 1
    use_tfb: False
    metrics: [ 'acc', 'rmse' ]
    seed: 42
    gpu: 0
  model_config:
    hidden_size: 32
    time_emb_size: 16
    num_layers: 2
    num_heads: 2
    mc_num_sample_per_step: 20
    sharing_param_layer: False
    loss_integral_num_sample_per_step: 20
    dropout: 0.0
    use_ln: False
    gpu: 0

RMTPP_eval_hawkes:
  base_config:
    stage: eval
    backend: torch
    dataset_id: hawkes_local
    runner_id: std_tpp
    model_id: RMTPP
    base_dir: ./checkpoints/
  trainer_config:
    batch_size: 32
    max_epoch: 1
    shuffle: False
    optimizer: adam
    learning_rate: 1.e-3
    valid_freq: 1
    use_tfb: False
    metrics: [ 'acc', 'rmse' ]
    seed: 42
    gpu: 0
  model_config:
    hidden_size: 32
    time_emb_size: 16
    num_layers: 2
    num_heads: 2
    mc_num_sample_per_step: 20
    sharing_param_layer: False
    loss_integral_num_sample_per_step: 20
    dropout: 0.0
    use_ln: False
    gpu: 0

ODETPP_train_hawkes:
  base_config:
    stage: train
    backend: torch
    dataset_id: hawkes_local
    runner_id: std_tpp
    model_id: ODETPP
    base_dir: ./checkpoints/
  trainer_config:
    batch_size: 8
    max_epoch: 3
    shuffle: False
    optimizer: adam
    learning_rate: 1.e-2
    valid_freq: 1
    use_tfb: False
    metrics: [ 'acc', 'rmse' ]
    seed: 42
    gpu: 0
  model_config:
    hidden_size: 8
    time_emb_size: 4
    num_layers: 1
    sharing_param_layer: False
    loss_integral_num_sample_per_step: 10
    dropout: 0.0
    use_ln: False
    gpu: 0
    model_specs:
      ode_num_sample_per_step: 2
      time_factor: 10
    thinning:
      num_seq: 5
      num_sample: 1
      num_exp: 100
      look_ahead_time: 5
      patience_counter: 3
      over_sample_rate: 5
      num_samples_boundary: 5
      dtime_max: 5

ODETPP_eval_hawkes:
  base_config:
    stage: eval
    backend: torch
    dataset_id: hawkes_local
    runner_id: std_tpp
    model_id: ODETPP
    base_dir: ./checkpoints/
  trainer_config:
    batch_size: 8
    max_epoch: 1
    shuffle: False
    optimizer: adam
    learning_rate: 1.e-2
    valid_freq: 1
    use_tfb: False
    metrics: [ 'acc', 'rmse' ]
    seed: 42
    gpu: 0
  model_config:
    hidden_size: 8
    time_emb_size: 4
    num_layers: 1
    sharing_param_layer: False
    loss_integral_num_sample_per_step: 10
    dropout: 0.0
    use_ln: False
    gpu: 0
    model_specs:
      ode_num_sample_per_step: 2
      time_factor: 10
    thinning:
      num_seq: 5
      num_sample: 1
      num_exp: 100
      look_ahead_time: 5
      patience_counter: 3
      over_sample_rate: 5
      num_samples_boundary: 5
      dtime_max: 5


